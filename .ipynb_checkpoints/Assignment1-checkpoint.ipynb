{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGc4qaTjPrU"
   },
   "source": [
    "Welcome to assignment 1.                                                       \n",
    "\n",
    "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s4K2S-dhTnF"
   },
   "source": [
    "\n",
    "\n",
    "# Task 1: Feature Generation (15%)\n",
    "# Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
    "# This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
    "# Allocate 10% of the data for validation.\n",
    "\n",
    "# Insert your code here for Task 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "94c_H8Yv7IDs"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set feature vectors shape: 702\n",
      "Validation set feature vectors shape: 78\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset folder â€” modify to match the local dowload location of the dataset on your machine.\n",
    "dataset_path = \"C:\\\\Users\\\\brian\\\\Downloads\\\\train-20240221T231820Z-001\\\\train\"\n",
    "\n",
    "# List to store image paths and labels\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over labeled folders (A to T)\n",
    "for label in os.listdir(dataset_path):\n",
    "    label_folder = os.path.join(dataset_path, label)\n",
    "    \n",
    "    # Iterate over images in each labeled folder\n",
    "    for image_name in os.listdir(label_folder):\n",
    "        image_path = os.path.join(label_folder, image_name)\n",
    "        all_image_paths.append(image_path)\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Split the data into training and validation sets (90% training, 10% validation)\n",
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n",
    "    all_image_paths, all_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained DenseNet model\n",
    "model = densenet121(pretrained=True)\n",
    "\n",
    "# Remove the classification layer (last fully connected layer)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Add a global average pooling layer\n",
    "model.add_module('global_avg_pool', torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the image preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features for a given image path\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_var = Variable(input_batch)\n",
    "    features = model(input_var)\n",
    "    feature_vector = features.squeeze().detach().numpy()\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# Extract features for training set\n",
    "train_features = [extract_features(image_path) for image_path in train_image_paths]\n",
    "\n",
    "# Extract features for validation set\n",
    "val_features = [extract_features(image_path) for image_path in val_image_paths]\n",
    "\n",
    "# Now 'train_features' and 'val_features' contain the features from the last fully connected layer of DenseNet\n",
    "print(\"Training set feature vectors shape:\", len(train_features))\n",
    "print(\"Validation set feature vectors shape:\", len(val_features))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DguMbSShmHT"
   },
   "source": [
    "# Task 2: High Bias Classification Method (5%)\n",
    "# Choose a classification method and let is have a high bias.\n",
    "# Train it on the generated features and discuss why it is underfitting.\n",
    "\n",
    "# Insert your code here for Task 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "xG7aIh1lhpW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 92.31%\n",
      "Confusion Matrix:\n",
      "[[4 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 5 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 4 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 5 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 3 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 1 0 0 5 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 4 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 4 0 0 0 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 4 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0]\n",
      " [0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 6]]\n",
      "\n",
      "Discussion:\n",
      "The underfitting may occur because a linear classifier (Logistic Regression) has a simple decision boundary and may notcapture the complex relationships present in the high-dimensionalfeature space generated by the pre-trained DenseNet model. A high-bias model tends to oversimplify the underlying patternsin the data, leading to lower accuracy.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:460: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'train_features' and 'train_labels' are the features and labels for training set\n",
    "# Assuming 'val_features' and 'val_labels' are the features and labels for validation set\n",
    "\n",
    "# Convert labels to integers (assuming labels are strings)\n",
    "label_to_int = {label: idx for idx, label in enumerate(set(train_labels))}\n",
    "train_labels_int = [label_to_int[label] for label in train_labels]\n",
    "val_labels_int = [label_to_int[label] for label in val_labels]\n",
    "\n",
    "# Use K-Means for classification\n",
    "kmeans = KMeans(n_clusters=len(set(train_labels)), random_state=42)\n",
    "kmeans.fit(train_features)\n",
    "\n",
    "# Predict cluster assignments for validation set\n",
    "val_predictions = kmeans.predict(val_features)\n",
    "\n",
    "# Convert cluster assignments to labels\n",
    "cluster_to_label = {cluster: label for label, cluster in label_to_int.items()}\n",
    "val_labels_pred = [cluster_to_label[cluster] for cluster in val_predictions]\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels_int, val_predictions)\n",
    "\n",
    "# Discuss why it might be underfitting\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"K-Means is a simple algorithm that assumes spherical clusters with equal variance.\")\n",
    "print(\"It may underfit when the underlying data distribution is non-linear or has varying cluster shapes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR8MxxoGhpxF"
   },
   "source": [
    "# Task 3: High Variance Classification Method (5%)\n",
    "# Use the chosen classification method and let it have a high variance.\n",
    "# Train it on the generated features and discuss why it is overfitting.\n",
    "\n",
    "# Insert your code here for Task 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrsSDN_7huYB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzxSVPWXht-m"
   },
   "source": [
    "# Task 4: Balanced Classification Method (15%)\n",
    "# Use the chosen classification method and let it balance the bias and variance.\n",
    "# Train it on the generated features, possibly adjusting parameters.\n",
    "# Discuss insights into achieving balance.\n",
    "\n",
    "# Insert your code here for Task 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjgmSxk7h7vZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKRG3PfFh8Ot"
   },
   "source": [
    "# Task 5: K-Means Clustering (20%)\n",
    "# Apply K-Means clustering on the generated features.\n",
    "# Test with available labels and report accuracy.\n",
    "# Experiment with automated K and compare with manually set 20 clusters.\n",
    "\n",
    "# Insert your code here for Task 5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLuOkJyAh-mN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43sPfI7Jh-9p"
   },
   "source": [
    "# Task 6: Additional Clustering Algorithm (10%)\n",
    "# Choose another clustering algorithm and apply it on the features.\n",
    "# Test accuracy with available labels.\n",
    "\n",
    "# Insert your code here for Task 6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn9f41LWiCDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fPfoBsaiCXu"
   },
   "source": [
    "# Task 7: PCA for Classification Improvement (20%)\n",
    "# Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
    "# Assess if PCA improves outcomes and discuss the results.\n",
    "\n",
    "# Insert your code here for Task 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoOFXhdmiHeD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQqNra7eiHx-"
   },
   "source": [
    "# Task 8: Visualization and Analysis (10%)\n",
    "# Plot the features in a lower dimension using dimentinality reduction techniques.\n",
    "# Analyze the visual representation, identifying patterns or insights.\n",
    "\n",
    "# Insert your code here for Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npTL_NkjNdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

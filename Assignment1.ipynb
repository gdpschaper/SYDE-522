{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGc4qaTjPrU"
   },
   "source": [
    "Welcome to assignment 1.                                                       \n",
    "\n",
    "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s4K2S-dhTnF"
   },
   "source": [
    "\n",
    "\n",
    "# Task 1: Feature Generation (15%)\n",
    "# Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
    "# This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
    "# Allocate 10% of the data for validation.\n",
    "\n",
    "# Insert your code here for Task 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "94c_H8Yv7IDs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/opt/homebrew/lib/python3.11/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set feature vectors shape: 702\n",
      "Validation set feature vectors shape: 78\n",
      "Training set feature vectors:\n",
      "Instance 1: [ 5.6815566e-04  5.7970881e-03  2.6177713e-03 ...  5.1698887e-01\n",
      "  4.0688884e-01 -5.7975239e-01] - Label: D\n",
      "Instance 2: [ 4.7536640e-04  1.0388358e-02  2.8466193e-03 ...  1.4883754e+00\n",
      " -1.6424663e-02 -6.4177150e-01] - Label: A\n",
      "Instance 3: [ 5.2169157e-04  1.2616204e-02  1.1426058e-04 ... -2.1618327e-01\n",
      " -2.1554305e-01 -2.0022346e-01] - Label: I\n",
      "Instance 4: [ 9.1784765e-05  7.4622752e-03  1.3906002e-03 ...  8.3241981e-01\n",
      " -6.6252053e-01 -1.7127241e-01] - Label: L\n",
      "Instance 5: [ 8.1051076e-05  1.0962947e-03  5.3642306e-04 ... -4.1521978e-01\n",
      "  9.8362891e-03  1.8082334e-02] - Label: E\n",
      "\n",
      "Validation set feature vectors:\n",
      "Instance 1: [ 2.95502279e-04  2.00712564e-03 -1.01288955e-04 ... -2.95696110e-01\n",
      "  4.95025903e-01  1.83987677e-01] - Label: E\n",
      "Instance 2: [0.00052962 0.00701606 0.00043407 ... 0.20975111 0.01167713 0.289521  ] - Label: E\n",
      "Instance 3: [ 4.0282198e-04  4.8080231e-03  2.3700518e-03 ... -3.0305220e-02\n",
      "  9.1994482e-01 -6.7561114e-01] - Label: D\n",
      "Instance 4: [-3.2278316e-05  3.0059109e-03  6.0406921e-04 ...  1.3258350e-02\n",
      "  8.7250143e-02  1.4175718e-01] - Label: B\n",
      "Instance 5: [ 4.4352942e-04 -1.8392955e-03 -9.3414437e-04 ...  1.5487176e+00\n",
      "  2.5252230e-02  6.0707670e-01] - Label: C\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset folder â€” modify to match the local dowload location of the dataset on your machine.\n",
    "dataset_path = \"train\"\n",
    "\n",
    "# List to store image paths and labels\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over labeled folders (A to T)\n",
    "for label in os.listdir(dataset_path):\n",
    "    label_folder = os.path.join(dataset_path, label)\n",
    "    \n",
    "    # Iterate over images in each labeled folder\n",
    "    for image_name in os.listdir(label_folder):\n",
    "        image_path = os.path.join(label_folder, image_name)\n",
    "        all_image_paths.append(image_path)\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Split the data into training and validation sets (90% training, 10% validation)\n",
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n",
    "    all_image_paths, all_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained DenseNet model\n",
    "model = densenet121(pretrained=True)\n",
    "\n",
    "# Remove the classification layer (last fully connected layer)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Add a global average pooling layer\n",
    "model.add_module('global_avg_pool', torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the image preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features for a given image path\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_var = Variable(input_batch)\n",
    "    features = model(input_var)\n",
    "    feature_vector = features.squeeze().detach().numpy()\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# Extract features for training set\n",
    "train_features = [extract_features(image_path) for image_path in train_image_paths]\n",
    "\n",
    "# Extract features for validation set\n",
    "val_features = [extract_features(image_path) for image_path in val_image_paths]\n",
    "\n",
    "# Now 'train_features' and 'val_features' contain the features from the last fully connected layer of DenseNet\n",
    "print(\"Training set feature vectors shape:\", len(train_features))\n",
    "print(\"Validation set feature vectors shape:\", len(val_features))\n",
    "\n",
    "# Print the first few feature vectors and labels for training set\n",
    "print(\"Training set feature vectors:\")\n",
    "for i in range(min(5, len(train_features))):\n",
    "    print(f\"Instance {i+1}: {train_features[i]} - Label: {train_labels[i]}\")\n",
    "\n",
    "# Print the first few feature vectors and labels for validation set\n",
    "print(\"\\nValidation set feature vectors:\")\n",
    "for i in range(min(5, len(val_features))):\n",
    "    print(f\"Instance {i+1}: {val_features[i]} - Label: {val_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DguMbSShmHT"
   },
   "source": [
    "# Task 2: High Bias Classification Method (5%)\n",
    "# Choose a classification method and let is have a high bias.\n",
    "# Train it on the generated features and discuss why it is underfitting.\n",
    "\n",
    "# Insert your code here for Task 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "xG7aIh1lhpW3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.00%\n",
      "K-Means is a simple algorithm that assumes spherical clusters with equal variance.\n",
      "It may underfit when the underlying data distribution is non-linear or has varying cluster shapes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'train_features' and 'train_labels' are the features and labels for training set\n",
    "# Assuming 'val_features' and 'val_labels' are the features and labels for validation set\n",
    "\n",
    "# Convert labels to integers (assuming labels are strings)\n",
    "label_to_int = {label: idx for idx, label in enumerate(set(train_labels))}\n",
    "train_labels_int = [label_to_int[label] for label in train_labels]\n",
    "val_labels_int = [label_to_int[label] for label in val_labels]\n",
    "\n",
    "# Use K-Means for classification\n",
    "kmeans = KMeans(n_clusters=len(set(train_labels)), random_state=42)\n",
    "kmeans.fit(train_features)\n",
    "\n",
    "# Predict cluster assignments for validation set\n",
    "val_predictions = kmeans.predict(val_features)\n",
    "\n",
    "# Convert cluster assignments to labels\n",
    "cluster_to_label = {cluster: label for label, cluster in label_to_int.items()}\n",
    "val_labels_pred = [cluster_to_label[cluster] for cluster in val_predictions]\n",
    "\n",
    "#print(val_labels_int, val_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels_int, val_predictions)\n",
    "\n",
    "# Discuss why it might be underfitting\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"K-Means is a simple algorithm that assumes spherical clusters with equal variance.\")\n",
    "print(\"It may underfit when the underlying data distribution is non-linear or has varying cluster shapes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR8MxxoGhpxF"
   },
   "source": [
    "# Task 3: High Variance Classification Method (5%)\n",
    "# Use the chosen classification method and let it have a high variance.\n",
    "# Train it on the generated features and discuss why it is overfitting.\n",
    "\n",
    "# Insert your code here for Task 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrsSDN_7huYB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzxSVPWXht-m"
   },
   "source": [
    "# Task 4: Balanced Classification Method (15%)\n",
    "# Use the chosen classification method and let it balance the bias and variance.\n",
    "# Train it on the generated features, possibly adjusting parameters.\n",
    "# Discuss insights into achieving balance.\n",
    "\n",
    "# Insert your code here for Task 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjgmSxk7h7vZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKRG3PfFh8Ot"
   },
   "source": [
    "# Task 5: K-Means Clustering (20%)\n",
    "# Apply K-Means clustering on the generated features.\n",
    "# Test with available labels and report accuracy.\n",
    "# Experiment with automated K and compare with manually set 20 clusters.\n",
    "\n",
    "# Insert your code here for Task 5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VLuOkJyAh-mN"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using KMeans where k=20:\n",
      "The homogeneity is 0.84\n",
      "The completeness is 0.79\n",
      "The v_score is 0.82\n",
      "\n",
      "Using automated KMeans where k=7:\n",
      "The homogeneity is 0.88\n",
      "The completeness is 0.56\n",
      "The v_score is 0.69\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "\n",
    "letter_to_int = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"J\": 9,\n",
    "    \"K\": 10,\n",
    "    \"L\": 11,\n",
    "    \"M\": 12,\n",
    "    \"N\": 13,\n",
    "    \"O\": 14,\n",
    "    \"P\": 15,\n",
    "    \"Q\": 16,\n",
    "    \"R\": 17,\n",
    "    \"S\": 18,\n",
    "    \"T\": 19\n",
    "}\n",
    "\n",
    "def print_cluster_results(homogeneity_completeness_v_score):\n",
    "    print(\"The homogeneity is \" + str(round(homogeneity_completeness_v_score[0], 2)))\n",
    "    print(\"The completeness is \" + str(round(homogeneity_completeness_v_score[1], 2)))\n",
    "    print(\"The v_score is \" + str(round(homogeneity_completeness_v_score[2], 2)))\n",
    "\n",
    "# Run KMeans algorithm with 20 clusters as we are have 20 classes\n",
    "kmeans = KMeans(n_clusters=20, random_state=0, n_init=\"auto\").fit(train_features)\n",
    "predicitions = kmeans.predict(train_features)\n",
    "\n",
    "int_train_labels = [letter_to_int[label] for label in train_labels]\n",
    "\n",
    "homogeneity_completeness_v_score = homogeneity_completeness_v_measure(predicitions, int_train_labels)\n",
    "\n",
    "print(\"Using KMeans where k=20:\")\n",
    "print_cluster_results(homogeneity_completeness_v_score)\n",
    "print()\n",
    "\n",
    "\n",
    "# Allow KMeans algorithm to automatically determine the number of classes\n",
    "kmeans = KMeans(random_state=0, n_init=\"auto\").fit(train_features)\n",
    "predicitions = kmeans.predict(train_features)\n",
    "\n",
    "int_train_labels = [letter_to_int[label] for label in train_labels]\n",
    "\n",
    "homogeneity_completeness_v_score = homogeneity_completeness_v_measure(predicitions, int_train_labels)\n",
    "\n",
    "print(\"Using automated KMeans where k=\" + str(max(predicitions)) + \":\")\n",
    "print_cluster_results(homogeneity_completeness_v_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43sPfI7Jh-9p"
   },
   "source": [
    "# Task 6: Additional Clustering Algorithm (10%)\n",
    "# Choose another clustering algorithm and apply it on the features.\n",
    "# Test accuracy with available labels.\n",
    "\n",
    "# Insert your code here for Task 6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Nn9f41LWiCDr"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using automated DBSCAN with maxmum radius of 12 and minimum samples per cluster of 2 images\n",
      "The homogeneity is 0.65\n",
      "The completeness is 0.32\n",
      "The v_score is 0.43\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.metrics.cluster import homogeneity_completeness_v_measure\n",
    "\n",
    "letter_to_int = {\n",
    "    \"A\": 0,\n",
    "    \"B\": 1,\n",
    "    \"C\": 2,\n",
    "    \"D\": 3,\n",
    "    \"E\": 4,\n",
    "    \"F\": 5,\n",
    "    \"G\": 6,\n",
    "    \"H\": 7,\n",
    "    \"I\": 8,\n",
    "    \"J\": 9,\n",
    "    \"K\": 10,\n",
    "    \"L\": 11,\n",
    "    \"M\": 12,\n",
    "    \"N\": 13,\n",
    "    \"O\": 14,\n",
    "    \"P\": 15,\n",
    "    \"Q\": 16,\n",
    "    \"R\": 17,\n",
    "    \"S\": 18,\n",
    "    \"T\": 19\n",
    "}\n",
    "\n",
    "def print_cluster_results(homogeneity_completeness_v_score):\n",
    "    print(\"The homogeneity is \" + str(round(homogeneity_completeness_v_score[0], 2)))\n",
    "    print(\"The completeness is \" + str(round(homogeneity_completeness_v_score[1], 2)))\n",
    "    print(\"The v_score is \" + str(round(homogeneity_completeness_v_score[2], 2)))\n",
    "\n",
    "MAXIMUM_RADIUS = 12\n",
    "MINIMUM_SAMPLES = 2\n",
    "\n",
    "# Run DBSCAN algorithm\n",
    "clusters = DBSCAN(eps=12, min_samples=2).fit(train_features)\n",
    "predicitions = clusters.labels_\n",
    "\n",
    "int_train_labels = [letter_to_int[label] for label in train_labels]\n",
    "\n",
    "homogeneity_completeness_v_score = homogeneity_completeness_v_measure(predicitions, int_train_labels)\n",
    "\n",
    "print(\"Using automated DBSCAN with maxmum radius of \" + str(MAXIMUM_RADIUS) + \\\n",
    "    \" and minimum samples per cluster of \" + str(MINIMUM_SAMPLES) + \" images\")\n",
    "print_cluster_results(homogeneity_completeness_v_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fPfoBsaiCXu"
   },
   "source": [
    "# Task 7: PCA for Classification Improvement (20%)\n",
    "# Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
    "# Assess if PCA improves outcomes and discuss the results.\n",
    "\n",
    "# Insert your code here for Task 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoOFXhdmiHeD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQqNra7eiHx-"
   },
   "source": [
    "# Task 8: Visualization and Analysis (10%)\n",
    "# Plot the features in a lower dimension using dimentinality reduction techniques.\n",
    "# Analyze the visual representation, identifying patterns or insights.\n",
    "\n",
    "# Insert your code here for Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npTL_NkjNdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

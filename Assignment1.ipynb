{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DgGc4qaTjPrU"
   },
   "source": [
    "Welcome to assignment 1.                                                       \n",
    "\n",
    "We are using pathology images for our first assignment please download data from this link https://drive.google.com/drive/folders/10dUOzcPR-PQwfFYcHk5gsLjIjSorQ32Q?usp=sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4s4K2S-dhTnF"
   },
   "source": [
    "\n",
    "\n",
    "# Task 1: Feature Generation (15%)\n",
    "# Use and run the following code (a deep network) to generate features from a set of training images. For this assignment, you do not need to know how the deep network is working here to extract features.\n",
    "# This code extracts the features of image T4.tif (in the T folder of dataset). Modify the code so that it iterates over all images of the dataset and extracts their features.\n",
    "# Allocate 10% of the data for validation.\n",
    "\n",
    "# Insert your code here for Task 1\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "94c_H8Yv7IDs",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set feature vectors shape: 702\n",
      "Validation set feature vectors shape: 78\n",
      "Training set feature vectors:\n",
      "Instance 1: [ 3.6497071e-04  5.1469303e-04  2.3895486e-03 ...  1.3170384e+00\n",
      " -4.5840797e-01 -3.8190687e-01] - Label: N\n",
      "Instance 2: [ 2.9535004e-04  1.8173460e-03 -6.7726779e-04 ...  3.1749940e-01\n",
      " -3.9225417e-01  3.6234949e-02] - Label: G\n",
      "Instance 3: [ 1.6402092e-04  1.0557906e-02 -1.6252657e-03 ...  7.0413810e-01\n",
      "  2.2983617e-01 -5.0651515e-01] - Label: B\n",
      "Instance 4: [ 0.00031655  0.00800259  0.00266579 ...  0.14289752 -0.08327961\n",
      " -0.10008601] - Label: S\n",
      "Instance 5: [ 3.0409099e-04  7.3866742e-03  5.1932072e-04 ...  1.0816039e+00\n",
      " -3.3889934e-01  4.2816776e-01] - Label: P\n",
      "\n",
      "Validation set feature vectors:\n",
      "Instance 1: [ 3.6549233e-04  6.7210454e-03  8.5626799e-04 ...  7.1736246e-01\n",
      " -2.8375876e-01 -6.8495661e-01] - Label: P\n",
      "Instance 2: [-1.0506088e-04  3.5598530e-03  3.3325888e-03 ...  9.3878144e-03\n",
      " -5.0184292e-01 -2.4772014e-01] - Label: P\n",
      "Instance 3: [ 0.00043349  0.00455786  0.00139878 ...  0.1549968  -0.32889074\n",
      "  0.03740197] - Label: N\n",
      "Instance 4: [2.6525528e-04 6.0476200e-03 1.5361963e-03 ... 5.4184753e-01 1.5195465e-01\n",
      " 1.0369193e-01] - Label: Q\n",
      "Instance 5: [4.8593120e-04 3.3365670e-03 6.1708211e-04 ... 2.2419028e+00 2.3293224e-01\n",
      " 1.0487540e+00] - Label: M\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import densenet121\n",
    "from torch.autograd import Variable\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Set the path to the dataset folder â€” modify to match the local dowload location of the dataset on your machine.\n",
    "dataset_path = \"C:\\\\Users\\\\brian\\\\Downloads\\\\SYDE 522\\\\train-20240221T231820Z-001\\\\train\"\n",
    "\n",
    "# List to store image paths and labels\n",
    "all_image_paths = []\n",
    "all_labels = []\n",
    "\n",
    "# Iterate over labeled folders (A to T)\n",
    "for label in os.listdir(dataset_path):\n",
    "    label_folder = os.path.join(dataset_path, label)\n",
    "    \n",
    "    # Iterate over images in each labeled folder\n",
    "    for image_name in os.listdir(label_folder):\n",
    "        image_path = os.path.join(label_folder, image_name)\n",
    "        all_image_paths.append(image_path)\n",
    "        all_labels.append(label)\n",
    "\n",
    "# Split the data into training and validation sets (90% training, 10% validation)\n",
    "train_image_paths, val_image_paths, train_labels, val_labels = train_test_split(\n",
    "    all_image_paths, all_labels, test_size=0.1, random_state=42\n",
    ")\n",
    "\n",
    "# Load pre-trained DenseNet model\n",
    "model = densenet121(pretrained=True)\n",
    "\n",
    "# Remove the classification layer (last fully connected layer)\n",
    "model = torch.nn.Sequential(*list(model.children())[:-1])\n",
    "\n",
    "# Add a global average pooling layer\n",
    "model.add_module('global_avg_pool', torch.nn.AdaptiveAvgPool2d(1))\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "model.eval()\n",
    "\n",
    "# Define the image preprocessing pipeline\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features for a given image path\n",
    "def extract_features(image_path):\n",
    "    image = Image.open(image_path)\n",
    "    input_tensor = preprocess(image)\n",
    "    input_batch = input_tensor.unsqueeze(0)\n",
    "    input_var = Variable(input_batch)\n",
    "    features = model(input_var)\n",
    "    feature_vector = features.squeeze().detach().numpy()\n",
    "    return feature_vector\n",
    "\n",
    "\n",
    "# Extract features for training set\n",
    "train_features = [extract_features(image_path) for image_path in train_image_paths]\n",
    "\n",
    "# Extract features for validation set\n",
    "val_features = [extract_features(image_path) for image_path in val_image_paths]\n",
    "\n",
    "# Now 'train_features' and 'val_features' contain the features from the last fully connected layer of DenseNet\n",
    "print(\"Training set feature vectors shape:\", len(train_features))\n",
    "print(\"Validation set feature vectors shape:\", len(val_features))\n",
    "\n",
    "# Print the first few feature vectors and labels for training set\n",
    "print(\"Training set feature vectors:\")\n",
    "for i in range(min(5, len(train_features))):\n",
    "    print(f\"Instance {i+1}: {train_features[i]} - Label: {train_labels[i]}\")\n",
    "\n",
    "# Print the first few feature vectors and labels for validation set\n",
    "print(\"\\nValidation set feature vectors:\")\n",
    "for i in range(min(5, len(val_features))):\n",
    "    print(f\"Instance {i+1}: {val_features[i]} - Label: {val_labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2DguMbSShmHT"
   },
   "source": [
    "# Task 2: High Bias Classification Method (5%)\n",
    "# Choose a classification method and let is have a high bias.\n",
    "# Train it on the generated features and discuss why it is underfitting.\n",
    "\n",
    "# Insert your code here for Task 2\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "xG7aIh1lhpW3"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\brian\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1412: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
      "  super()._check_params_vs_input(X, default_n_init=10)\n",
      "C:\\Users\\brian\\anaconda3\\Lib\\site-packages\\sklearn\\cluster\\_kmeans.py:1436: UserWarning: KMeans is known to have a memory leak on Windows with MKL, when there are less chunks than available threads. You can avoid it by setting the environment variable OMP_NUM_THREADS=3.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[12, 12, 3, 15, 18, 9, 17, 3, 5, 14, 17, 15, 13, 13, 13, 11, 3, 10, 4, 2, 10, 0, 2, 3, 8, 14, 1, 7, 16, 10, 7, 2, 11, 12, 13, 4, 1, 3, 19, 14, 19, 13, 11, 15, 9, 13, 19, 16, 10, 6, 18, 6, 10, 15, 9, 19, 17, 12, 1, 14, 4, 8, 15, 8, 8, 18, 5, 4, 18, 12, 5, 6, 5, 9, 19, 3, 5, 13] [17  9 11  8  1  5  2 11 16  4  2 16 12 12 12 18 11 17 13  1 17 14 15 16\n",
      "  3  4  6 19 14 17 19 15  3 19 13 13  6 11 17  4  0 12 18  8  5  9 17 17\n",
      " 17  7  1  7 17  8  5 17  2  0  6  4 13  3  8  3  3  1 16 19  1 17 16  7\n",
      " 16  5 17 11 16 12]\n",
      "Accuracy: 1.28%\n",
      "K-Means is a simple algorithm that assumes spherical clusters with equal variance.\n",
      "It may underfit when the underlying data distribution is non-linear or has varying cluster shapes.\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Assuming 'train_features' and 'train_labels' are the features and labels for training set\n",
    "# Assuming 'val_features' and 'val_labels' are the features and labels for validation set\n",
    "\n",
    "# Convert labels to integers (assuming labels are strings)\n",
    "label_to_int = {label: idx for idx, label in enumerate(set(train_labels))}\n",
    "train_labels_int = [label_to_int[label] for label in train_labels]\n",
    "val_labels_int = [label_to_int[label] for label in val_labels]\n",
    "\n",
    "# Use K-Means for classification\n",
    "kmeans = KMeans(n_clusters=len(set(train_labels)), random_state=42)\n",
    "kmeans.fit(train_features)\n",
    "\n",
    "# Predict cluster assignments for validation set\n",
    "val_predictions = kmeans.predict(val_features)\n",
    "\n",
    "# Convert cluster assignments to labels\n",
    "cluster_to_label = {cluster: label for label, cluster in label_to_int.items()}\n",
    "val_labels_pred = [cluster_to_label[cluster] for cluster in val_predictions]\n",
    "\n",
    "#print(val_labels_int, val_predictions)\n",
    "\n",
    "# Calculate accuracy\n",
    "accuracy = accuracy_score(val_labels_int, val_predictions)\n",
    "\n",
    "# Discuss why it might be underfitting\n",
    "print(f\"Accuracy: {accuracy:.2%}\")\n",
    "print(\"K-Means is a simple algorithm that assumes spherical clusters with equal variance.\")\n",
    "print(\"It may underfit when the underlying data distribution is non-linear or has varying cluster shapes.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kR8MxxoGhpxF"
   },
   "source": [
    "# Task 3: High Variance Classification Method (5%)\n",
    "# Use the chosen classification method and let it have a high variance.\n",
    "# Train it on the generated features and discuss why it is overfitting.\n",
    "\n",
    "# Insert your code here for Task 3\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TrsSDN_7huYB"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yzxSVPWXht-m"
   },
   "source": [
    "# Task 4: Balanced Classification Method (15%)\n",
    "# Use the chosen classification method and let it balance the bias and variance.\n",
    "# Train it on the generated features, possibly adjusting parameters.\n",
    "# Discuss insights into achieving balance.\n",
    "\n",
    "# Insert your code here for Task 4\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjgmSxk7h7vZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZKRG3PfFh8Ot"
   },
   "source": [
    "# Task 5: K-Means Clustering (20%)\n",
    "# Apply K-Means clustering on the generated features.\n",
    "# Test with available labels and report accuracy.\n",
    "# Experiment with automated K and compare with manually set 20 clusters.\n",
    "\n",
    "# Insert your code here for Task 5\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VLuOkJyAh-mN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "43sPfI7Jh-9p"
   },
   "source": [
    "# Task 6: Additional Clustering Algorithm (10%)\n",
    "# Choose another clustering algorithm and apply it on the features.\n",
    "# Test accuracy with available labels.\n",
    "\n",
    "# Insert your code here for Task 6\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Nn9f41LWiCDr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3fPfoBsaiCXu"
   },
   "source": [
    "# Task 7: PCA for Classification Improvement (20%)\n",
    "# Apply PCA on the features and then feed them to the best classification method in the above tasks.\n",
    "# Assess if PCA improves outcomes and discuss the results.\n",
    "\n",
    "# Insert your code here for Task 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RoOFXhdmiHeD"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQqNra7eiHx-"
   },
   "source": [
    "# Task 8: Visualization and Analysis (10%)\n",
    "# Plot the features in a lower dimension using dimentinality reduction techniques.\n",
    "# Analyze the visual representation, identifying patterns or insights.\n",
    "\n",
    "# Insert your code here for Task 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1npTL_NkjNdL"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
